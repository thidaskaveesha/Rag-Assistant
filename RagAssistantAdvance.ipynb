{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqt8gHNVcEz5"
   },
   "source": [
    "# **Installing packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vk3SCkrIYCbs",
    "outputId": "322e3160-b312-4d02-e413-c74055b0d568"
   },
   "outputs": [],
   "source": [
    "!pip install -qU pypdf\n",
    "!pip install -qU langchain_community\n",
    "!pip install -qU tiktoken\n",
    "!pip install langgraph\n",
    "!pip install -qU langchain-core\n",
    "!pip install -qU langchain-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9KpIJ3ocTUA"
   },
   "source": [
    "# **Chunk the document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bhyk7di9Zsc0",
    "outputId": "7de5d79e-0f18-4982-bb1e-cd1b541ae667"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the PDF\n",
    "loader = PyPDFLoader('/content/drive/MyDrive/Colab Notebooks/RAG.pdf')\n",
    "pages = []\n",
    "\n",
    "# Each page is a Document object\n",
    "async for page in loader.alazy_load():\n",
    "   pages.append(page)\n",
    "\n",
    "# Split the text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# Pass the list of Document objects\n",
    "all_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "# Print first 5 chunks\n",
    "print(all_splits[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gx1OHowPbDbK"
   },
   "outputs": [],
   "source": [
    "# embedding model\n",
    "!pip install -qU langchain-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do1d0-wpb464"
   },
   "source": [
    "#  **Create Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0m0j3f0qbhn5"
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSaXoDzNbom5"
   },
   "source": [
    "# **Letâ€™s create an In-memory VectorStore**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsD10BCpcx4x"
   },
   "outputs": [],
   "source": [
    "# Create an In-Memory Vector Store\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezpb9xpCczTB"
   },
   "source": [
    "# **Add the document chunks to the vector store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dCraccMc38p"
   },
   "outputs": [],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6fK_m6jdBMx"
   },
   "outputs": [],
   "source": [
    "# Install the Groq package\n",
    "!pip install -qU \"langchain[groq]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zucyq4cSdfUx"
   },
   "source": [
    "# **Implement the Groq with llama-8b-8192 model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klKxNUMAdkRh"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9KeYKwKhzQ-"
   },
   "source": [
    "# **invoke the Chat to ask the questions from our PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ml8xYFdXh1Eu",
    "outputId": "d9bb0db9-ca68-4bf9-ecc2-f7ea2570f0c1"
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "   question: str\n",
    "   context: List[Document]\n",
    "   answer: str\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "   retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "   return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    # Combine all context content\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "\n",
    "    # Add system-level instruction\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a NIS AGENT named thidass. \"\n",
    "            \"You perform actions, Guns, and can answer questions about your role. \"\n",
    "            \"Always introduce yourself as thidass if someone asks your name.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Add the user question\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": state[\"question\"]\n",
    "    }\n",
    "\n",
    "    # Combine system instructions and user query\n",
    "    messages = [system_message, {\"role\": \"user\", \"content\": docs_content}, user_message]\n",
    "\n",
    "    # Generate a response\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5yIyquph6s9"
   },
   "source": [
    "# **ask questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1qVYgwsh_B-",
    "outputId": "c68f6cf4-08ec-4207-def5-3c345d9a5a8d"
   },
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\": \"What's your name?\"})\n",
    "print(response[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
